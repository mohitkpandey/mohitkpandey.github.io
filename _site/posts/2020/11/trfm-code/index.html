

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Transformer Network in Pytorch from scratch - Mohit Pandey</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Mohit Pandey">
<meta property="og:title" content="Transformer Network in Pytorch from scratch">


  <link rel="canonical" href="https://mohitkpandey.github.io/posts/2020/11/trfm-code/">
  <meta property="og:url" content="https://mohitkpandey.github.io/posts/2020/11/trfm-code/">



  <meta property="og:description" content="Step by step implementation of “Attention is all you need” with animated explanations.This is a supplementary post to the medium article Transformers in Cheminformatics.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2021-06-22T00:00:00-05:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Mohit Pandey",
      "url" : "https://mohitkpandey.github.io",
      "sameAs" : null
    }
  </script>



  <meta name="google-site-verification" content="C-UVrmUCUMmXNs9Gy9W1ilDqiP2ZPhqb69o5LlGUhBo" />




<!-- end SEO -->


<link href="https://mohitkpandey.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Mohit Pandey Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://mohitkpandey.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://mohitkpandey.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://mohitkpandey.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://mohitkpandey.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://mohitkpandey.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://mohitkpandey.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://mohitkpandey.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://mohitkpandey.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://mohitkpandey.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://mohitkpandey.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://mohitkpandey.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://mohitkpandey.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://mohitkpandey.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://mohitkpandey.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://mohitkpandey.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://mohitkpandey.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://mohitkpandey.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://mohitkpandey.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://mohitkpandey.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://mohitkpandey.github.io/">Mohit Pandey</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://mohitkpandey.github.io/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://mohitkpandey.github.io/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://mohitkpandey.github.io/posts/">Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://mohitkpandey.github.io/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  
  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://mohitkpandey.github.io/images/profile.jpg" class="author__avatar" alt="Mohit Pandey">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Mohit Pandey</h3>
    <p class="author__bio">Ph.D. Candidate</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Contact</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker-alt" aria-hidden="true"></i> New York, NY</li>
      
      
      
      
        <li><a href="mailto:pandey.mohitk@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
        <li><a href="https://twitter.com/mkp0705"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
        <li><a href="https://www.linkedin.com/in/mohit-pandey-961337120/"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/diamondspark"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="http://orcid.org/0000-0002-2562-7155"><i class="ai ai-orcid ai-fw"></i> ORCID</a></li>
      
      
        <li><a href="https://scholar.google.com/citations?hl=en&user=oUhThscAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
        <li><a href="https://mohit-pandey.medium.com"><i class="fab fa-medium"></i> Medium</a></li>
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Transformer Network in Pytorch from scratch">
    <meta itemprop="description" content="Step by step implementation of “Attention is all you need” with animated explanations.This is a supplementary post to the medium article Transformers in Cheminformatics.">
    <meta itemprop="datePublished" content="June 22, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Transformer Network in Pytorch from scratch
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  8 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2021-06-22T00:00:00-05:00">June 22, 2021</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-align-justify"></i> On This Page</h4></header>
              <ul class="toc__menu">
  <li><a href="#code">Code</a>
    <ul>
      <li><a href="#single-headed-dot-scaled-attention">Single headed dot-scaled attention</a></li>
      <li><a href="#pointwise-feedforward-neural-network">Pointwise Feedforward Neural Network</a></li>
      <li><a href="#layernorm">LayerNorm</a></li>
      <li><a href="#residual-connection-add--norm">Residual Connection (Add &amp; Norm)</a></li>
      <li><a href="#positional-embedding">Positional Embedding</a></li>
      <li><a href="#encoder-layer">Encoder Layer</a></li>
      <li><a href="#encoder-stack-of-encoder-layers">Encoder (Stack of encoder layers)</a></li>
      <li><a href="#decoder-layer">Decoder Layer</a></li>
      <li><a href="#autoregression">Autoregression</a></li>
      <li><a href="#decoder-layer-1">Decoder layer</a></li>
      <li><a href="#decoder">Decoder</a></li>
      <li><a href="#transformer-network">Transformer Network</a></li>
    </ul>
  </li>
</ul>

            </nav>
          </aside>
        
        <p>Step by step implementation of “Attention is all you need” with animated explanations.</p>

<p>This is a supplementary post to the medium article <a href="https://medium.com/geekculture/transformers-in-cheminformatics-part-1-6ebda8f2781c">Transformers in Cheminformatics</a>. 
<!--more--></p>
<h1 id="code">Code</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">math</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1"># import seaborn
</span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="c1"># seaborn.set_context(context="talk")
</span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<h2 id="single-headed-dot-scaled-attention">Single headed dot-scaled attention</h2>

\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt {d_k}})V\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s">'./Images/self_attn.png'</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/files/images/transformer/output_2_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span><span class="n">value</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span><span class="n">dropout</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>  <span class="c1">#query:Q, key: K, value: V
</span>    <span class="n">dk</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span><span class="n">key</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#BxLxD
</span>    <span class="n">scaled_score</span> <span class="o">=</span> <span class="n">score</span><span class="o">/</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>
    <span class="c1">#Masking (optional) 
</span>    <span class="c1">#Increase score to very large negative number for tokens that are masked.
</span>    <span class="c1">#Such large negative number will have 0 exponentiation and hence their softmax will be 0 as well. 
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scaled_score</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_score</span><span class="p">,</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">#Optional: Dropout
</span>    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
    <span class="c1">#Z = enriched embedding 
</span>    <span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span><span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">attention</span>

    
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s">'./Images/multiheaded_attn.png'</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">850</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/files/images/transformer/output_4_0.png" alt="png" /></p>

<p><img src="/files/images/transformer/trfm_ip_op.gif" alt="SegmentLocal" title="segment" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">nheads</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadAttention</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">dmodel</span> <span class="o">%</span> <span class="n">nheads</span> <span class="o">==</span><span class="mi">0</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">dk</span> <span class="o">=</span> <span class="n">dmodel</span><span class="o">//</span><span class="n">nheads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nheads</span> <span class="o">=</span>  <span class="n">nheads</span>
        
        <span class="c1">#From the theory Wq linear layer should be (dmodel x dk)
</span>        <span class="c1">#But in implementation (we're using dmodel x dmodel) we will breakdown Wq into h heads later.
</span>        <span class="c1">#It can we shown that calculating 'nheads' small q_i's of BxLxdk dimension individually by feeding
</span>        <span class="c1">#key, query, value of dimension BxLxdk each is equivalent to 
</span>        <span class="c1">#calculating 1 big Wq of BxLxdmodel dimension and feeding in large X (BxLxdmodel) to get a large Q (BxLxdmodel)
</span>        <span class="c1">#then breaking Q into 'nheads' smaller q_i's of dimension BxLxdk each.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">Wq</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dmodel</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Wk</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dmodel</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Wv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dmodel</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Wo</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dmodel</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout_value</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span> <span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Same mask applied to all of the nheads
</span>            <span class="n">mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
       
        <span class="c1">#Dim: q=k=v=x : (BxLxdmodel)
</span>        <span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Wk</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">Wq</span><span class="p">(</span><span class="n">query</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">Wv</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>  <span class="c1">#k,q,v = (BxLxdmodel)
</span>        
        <span class="c1">#Break k,q,v into nheads k_i's, q_i's and v_i's of dim (BxLxdk)
</span>        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">nheads</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">dk</span> <span class="p">)</span> <span class="c1">#(B,L,nheads,dk) (view -1: actual value for this dimension will be inferred so that the number of elements in the view matches the original number of elements.)
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">nheads</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">dk</span><span class="p">)</span>  
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">nheads</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">dk</span><span class="p">)</span>
        
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># (B,L,nheads,dk) --&gt; (B,nheads,L,dk)
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span><span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1">#Calculate self attention and enriched embedding z_i's. 
</span>        <span class="c1">#All z_i's are channeled together in 1 large z matrix below
</span>        <span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">self_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">mask</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout_value</span><span class="p">)</span>  <span class="c1">#z : (B,nheads,L,dk), attn: (B,nheads,L,L)
</span>        
        <span class="c1">#Reshape z:(B,nheads,L,dk) --&gt;z_concat (B,L,nheads*dk) to refelect the affect of concatenation as shown in figure
</span>        <span class="n">z_concat</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#z:(B,nheads,L,dk) --&gt; z_concat: (B,L,nheads,dk)
</span>        <span class="n">z_concat</span> <span class="o">=</span> <span class="n">z_concat</span><span class="p">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1">#z_concat: (B,L,nheads,dk) --&gt; z_concat: (1,B*L*nheads*dk)
</span>        <span class="n">z_concat</span> <span class="o">=</span> <span class="n">z_concat</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">nheads</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">dk</span><span class="p">)</span> <span class="c1">#z_concat: (1,B*L*nheads*dk) --&gt; z_concat (B,L,nheads*dk)
</span>        
        <span class="c1">#Project z_concat with linear layer (Wo) to get final enriched embedding z_enriched as shown in figure
</span>        <span class="c1">#z_concat (B,L,nheads*dk) --&gt; z_enriched(B,L,dmodel)
</span>        <span class="n">z_enriched</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Wo</span><span class="p">(</span><span class="n">z_concat</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z_enriched</span>
</code></pre></div></div>

<h2 id="pointwise-feedforward-neural-network">Pointwise Feedforward Neural Network</h2>
<p>$FFN(x) = max(0,xW_1+b_1)W_2 + b_2$</p>

<p><img src="/files/images/transformer/Pointwise-ff-nn.gif" alt="SegmentLocal" title="segment" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PointwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span> <span class="n">d_linear</span><span class="p">,</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PointwiseFeedForward</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">d_linear</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_linear</span><span class="p">,</span> <span class="n">dmodel</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<h2 id="layernorm">LayerNorm</h2>
<p>\(X=\{x_1,x_2......x_m\} \\
% Y =\{y_1,y_2......y_m\} \\
x_i = \{x_{i,1},x_{i,2},.........,x_{i,K}\}\)</p>

<p>LayerNorm normalizes each $x_i$ across all its (K) features such that each sample $x_i$ has 0 mean and unit variance.</p>

\[\mu_i = \frac{1}{K}\sum_{k=1}^K x_{i,k} \\
\sigma_i^2 = \frac{1}{K}\sum_{k=1}^K (x_{i,k}-\mu_i)^2\]

\[\hat{x}_{i,k}= \frac{x_{i,k}-\mu_i}{\sqrt{\sigma_i^2 + \epsilon}}\]

<p>And finally the output of a Layernorm layer $y_i = LN_{\beta,\gamma}(x_i)$</p>

\[y_i = \gamma \hat{x_i} + \beta\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">):</span>
        <span class="s">'features = number of features along which to normalize </span><span class="se">\
</span><span class="s">        in the given input vector/matrix = dmodel'</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="c1">#calculate mean and std across the last dimension.
</span>        <span class="c1">#this will enforce that mean and std are calculated across
</span>        <span class="c1">#all features of a fed in example.
</span>        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">x</span><span class="o">-</span><span class="n">mean</span><span class="o">/</span><span class="p">(</span><span class="n">std</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="c1">#for numerical stability, we skip sqrt in denominator
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">x_hat</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">beta</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<h2 id="residual-connection-add--norm">Residual Connection (Add &amp; Norm)</h2>
<p>Residual Connection followed by layerNorm</p>

\[Add\_and\_Norm(Sublayer(x)) = LayerNorm(x+Dropout(Sublayer(x)))\]

<p>With the Residual connection and LayerNorm, the complete Transformer block looks as following
<img src="/files/images/transformer/full_trfm_block.gif" alt="SegmentLocal" title="segment" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AddandNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AddandNorm</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">sublayer_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sublayer_output</span><span class="p">))</span>

</code></pre></div></div>

<h2 id="positional-embedding">Positional Embedding</h2>
<p>Input representation:        $\hspace{23mm}\textbf{X_input} \in R^{L \times dmodel}$<br />
Positional Embedding: $\hspace{23mm}\textbf{P} \in R^{L \times dmodel}$<br />
Updated Embedding: $\hspace{23mm}\textbf{X} = \textbf{X_input} + \textbf{P}$<br />
Elements in $\textbf{P}$ are calculated as following
\(p_{i,2j}= sin\bigg(\frac{i}{10000^{\frac{2j}{dmodel}}}\bigg) = sin(\theta)\)</p>

\[p_{i,2j+1}= cos\bigg(\frac{i}{10000^{\frac{2j}{dmodel}}}\bigg) = cos(\theta)\]

<p><img src="/files/images/transformer/Transformer_Encoder_Block_w_Positional_Embeddings.gif" width="700" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="n">device</span><span class="p">,</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEmbeddingitionalEmbedding</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        
        <span class="c1">#i is a max_len dimensional vector, so that we can store a positional embedding
</span>        <span class="c1">#value corresponding to each token in sequence (Character in SMILES)
</span>        <span class="n">theta_numerator</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">theta_denominator</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span><span class="o">/</span><span class="n">dmodel</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_numerator</span><span class="o">/</span><span class="n">theta_denominator</span>
        
        <span class="c1">#Create a large P tensor to hold position embedding value for each token in the sequence
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">maxlen</span><span class="p">,</span><span class="n">dmodel</span><span class="p">))</span>
        <span class="c1">#Update even column ids in P with sin(theta) and odd column ids with cos(theta)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">P</span><span class="p">[:,</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">P</span><span class="p">[:,</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># x.shape[1] gives the length of input sequence
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">P</span><span class="p">[:,</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],:]</span>  
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        
</code></pre></div></div>

<h2 id="encoder-layer">Encoder Layer</h2>
<p>We are ready to build a single encoder layer by combining Residual connection, <br />
Layernorm, multiheaded attention block and position-wise feedforward network 
<img src="/files/images/transformer/encoder_block_raster.png.001.png" width="400" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">dmodel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">dlinear</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">multihead_attn</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">add_norm1</span> <span class="o">=</span> <span class="n">AddandNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pw_ffn</span> <span class="o">=</span> <span class="n">PointwiseFeedForward</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dlinear</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">add_norm2</span> <span class="o">=</span> <span class="n">AddandNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">'The input to the encoderlayer is either the embedding for first encoder layer </span><span class="se">\
</span><span class="s">         or representions from previous layer. We use key=query=value = input (x) to feed </span><span class="se">\
</span><span class="s">         into the multiheaded attention block within encoder layer'</span>
        <span class="n">multihead_attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">multihead_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">addnorm1_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">multihead_attn_output</span><span class="p">)</span>
        <span class="n">pwffn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pw_ffn</span><span class="p">(</span><span class="n">addnorm1_out</span><span class="p">)</span>
        <span class="n">addnorm2_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_norm2</span><span class="p">(</span><span class="n">addnorm1_out</span><span class="p">,</span><span class="n">pwffn_outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">addnorm2_out</span>
        
</code></pre></div></div>

<h2 id="encoder-stack-of-encoder-layers">Encoder (Stack of encoder layers)</h2>
<p>Encoder stack is a sequential model with N encoder layers. Vaswani et al. used N=6 in their original paper.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">device</span><span class="p">,</span><span class="n">src_vocab</span><span class="p">,</span><span class="n">nlayers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span><span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">dmodel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                 <span class="n">dlinear</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dmodel</span> <span class="o">=</span> <span class="n">dmodel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">src_vocab</span><span class="p">),</span><span class="n">dmodel</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">encoder_stack</span><span class="p">.</span><span class="n">add_module</span><span class="p">(</span><span class="s">"Encoder_Layer_"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                                          <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">nhead</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dlinear</span><span class="p">,</span><span class="n">dropout</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pos_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">embedding</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dmodel</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_stack</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">attention_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">multihead_attn</span><span class="p">.</span><span class="n">attn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
                            
            
        
</code></pre></div></div>

<h2 id="decoder-layer">Decoder Layer</h2>
<p>View 1 <img src="/files/images/transformer/trfm_decoder_block.gif" width="400" /></p>

<p>View 2
<img src="/files/images/transformer/trfm_decoder_block_view2.gif" width="400" /></p>

<h2 id="autoregression">Autoregression</h2>
<p>Mask out subsequent positions for decoder use</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">autoregression_mask</span><span class="p">(</span><span class="n">nbatch</span><span class="p">,</span><span class="n">size</span><span class="p">):</span>
    <span class="s">"Mask out subsequent positions."</span>
    <span class="n">attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">nbatch</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="n">autoregression_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'uint8'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">autoregression_mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

<span class="k">print</span><span class="p">(</span><span class="n">autoregression_mask</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">).</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">autoregression_mask</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([1, 20, 20])
&lt;matplotlib.image.AxesImage at 0x7f2eba48b208&gt;
</code></pre></div></div>

<p><img src="/files/images/transformer/output_22_2.png" alt="png" /></p>

<h2 id="decoder-layer-1">Decoder layer</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">nheads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">dmodel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">dlinear</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="c1">#identifier to distinguish decoder layers in stacked decoder
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">multihead_attn1</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">add_norm1</span> <span class="o">=</span> <span class="n">AddandNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">multihead_en_de</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">add_norm2</span> <span class="o">=</span> <span class="n">AddandNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">pw_ffn</span> <span class="o">=</span> <span class="n">PointwiseFeedForward</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dlinear</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">add_norm3</span> <span class="o">=</span> <span class="n">AddandNorm</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">encoder_output</span><span class="p">,</span><span class="n">encoder_mask</span><span class="p">,</span><span class="n">decoder_state</span><span class="p">,</span><span class="n">time_step</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">nbatch</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">decoder_mask</span> <span class="o">=</span> <span class="n">autoregression_mask</span><span class="p">(</span><span class="n">nbatch</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">dmodel</span><span class="p">)</span> <span class="c1">#(B,L,L)
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">decoder_mask</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1">#when training, decoder has access to the entire output sequence
</span>            <span class="c1">#hence can take entire x (target sequence) as input. Obviously proper masking
</span>            <span class="c1">#of target sequence is needed to stop the decoder from seeing future tokens
</span>            <span class="n">mulithead_attn1_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">multihead_attn1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">decoder_mask</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#when validating, decoder hasn't produced the word beyond the time step it is 
</span>            <span class="c1">#processing. Decoding happens one word at a time during validation.
</span>            <span class="c1">#at t=0, input to multihead_attn1 
</span>            <span class="c1">#q,k,v = &lt;bos&gt; token, 
</span>            <span class="c1">#at t=1 and beyond, input to ith decoder block (from prev. decoder side) is whatever
</span>            <span class="c1">#was predicted at prev. time step and the ith decoder block's state at t-1 timestep. 
</span>            <span class="c1">#See figure above
</span>            <span class="k">if</span> <span class="n">time_step</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">mulithead_attn1_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">multihead_attn1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">decoder_mask</span><span class="p">)</span>
                <span class="c1">#update decoder state with current time step's state
</span>                <span class="n">decoder_state</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">x</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">decoder_query</span> <span class="o">=</span> <span class="n">x</span>
                <span class="n">decoder_key_value</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">decoder_state</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">i</span><span class="p">],</span><span class="n">x</span><span class="p">),</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">mulithead_attn1_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">multihead_attn1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">decoder_key_value</span><span class="p">,</span>
                                                              <span class="n">decoder_key_value</span><span class="p">,</span><span class="n">decoder_mask</span><span class="p">)</span>
                <span class="c1">#update decoder state with current time step's state
</span>                <span class="n">decoder_state</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">decoder_key_value</span>
            
            
        <span class="n">addnorm1_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_norm1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mulithead_attn1_output</span><span class="p">)</span>
    
        <span class="n">key</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">query</span> <span class="o">=</span> <span class="n">encoder_output</span><span class="p">,</span><span class="n">encoder_output</span><span class="p">,</span><span class="n">addnorm1_out</span>
        <span class="n">multihead_en_de_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">multihead_en_de</span><span class="p">(</span><span class="n">query</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">encoder_mask</span><span class="p">)</span>
        <span class="n">addnorm2_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_norm2</span><span class="p">(</span><span class="n">addnorm1_out</span><span class="p">,</span><span class="n">multihead_en_de_output</span><span class="p">)</span>
        
        <span class="n">pwffn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pw_ffn</span><span class="p">(</span><span class="n">addnorm2_out</span><span class="p">)</span>
        <span class="n">addnorm3_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_norm2</span><span class="p">(</span><span class="n">addnorm2_out</span><span class="p">,</span><span class="n">pwffn_outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">addnorm3_out</span><span class="p">,</span><span class="n">decoder_state</span>
        
</code></pre></div></div>

<h2 id="decoder">Decoder</h2>
<p>Decoder stack is also a sequential model like encoder with N layers. Vaswani et al. used N=6 in their original paper.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">device</span><span class="p">,</span><span class="n">tgt_vocab</span><span class="p">,</span><span class="n">nlayers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span><span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">dmodel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                 <span class="n">dlinear</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dmodel</span> <span class="o">=</span> <span class="n">dmodel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nlayers</span> <span class="o">=</span> <span class="n">nlayers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_vocab</span><span class="p">),</span><span class="n">dmodel</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="n">device</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">decoder_stack</span><span class="p">.</span><span class="n">add_module</span><span class="p">(</span><span class="s">"Decoder_Layer_"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                                          <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">nhead</span><span class="p">,</span><span class="n">dmodel</span><span class="p">,</span><span class="n">dropout</span><span class="p">,</span><span class="n">dlinear</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dmodel</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_vocab</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">encoder_output</span><span class="p">,</span><span class="n">encoder_mask</span><span class="p">,</span><span class="n">decoder_state</span><span class="p">,</span><span class="n">time_step</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pos_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">embedding</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dmodel</span><span class="p">))</span>
        <span class="c1">#To save attention weights from both multiheaded attention layers
</span>        <span class="c1">#for later visualization
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">att_wts_de</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">att_wts_en_de</span> <span class="o">=</span> <span class="p">[],[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_stack</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span><span class="n">decoder_state</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">encoder_output</span><span class="p">,</span><span class="n">encoder_mask</span><span class="p">,</span><span class="n">decoder_state</span><span class="p">,</span><span class="n">time_step</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">att_wts_de</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">multihead_attn1</span><span class="p">.</span><span class="n">attn</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">att_wts_en_de</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">multihead_en_de</span><span class="p">.</span><span class="n">attn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">decoder_state</span>
                            
            
        
</code></pre></div></div>

<h2 id="transformer-network">Transformer Network</h2>
<p><img src="/files/images/transformer/endoer_decoder_combined.gif" width="700" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">encoder</span><span class="p">,</span><span class="n">decoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">src_input</span><span class="p">,</span><span class="n">src_mask</span><span class="p">,</span><span class="n">tgt_input</span><span class="p">,</span><span class="n">decoder_state</span><span class="p">,</span><span class="n">time_step</span><span class="p">):</span>
        <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">src_input</span><span class="p">)</span>
        <span class="n">decoder_output</span><span class="p">,</span><span class="n">decoder_state</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">tgt_input</span><span class="p">,</span><span class="n">encoder_output</span><span class="p">,</span>
                                               <span class="n">encoder_mask</span><span class="p">,</span><span class="n">decoder_state</span><span class="p">,</span><span class="n">time_step</span><span class="p">)</span>
        
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">device</span><span class="p">,</span><span class="n">src_vocab</span><span class="p">,</span><span class="n">nlayers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span><span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">dmodel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                 <span class="n">dlinear</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">device</span><span class="p">,</span><span class="n">tgt_vocab</span><span class="p">,</span><span class="n">nlayers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span><span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">dmodel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                 <span class="n">dlinear</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">trfm_network</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span><span class="n">decoder</span><span class="p">)</span>
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://mohitkpandey.github.io/tags/#machine-translation" class="page__taxonomy-item" rel="tag">Machine Translation</a><span class="sep">, </span>
    
      
      
      <a href="https://mohitkpandey.github.io/tags/#nlp" class="page__taxonomy-item" rel="tag">NLP</a><span class="sep">, </span>
    
      
      
      <a href="https://mohitkpandey.github.io/tags/#pytorch" class="page__taxonomy-item" rel="tag">Pytorch</a><span class="sep">, </span>
    
      
      
      <a href="https://mohitkpandey.github.io/tags/#transformers" class="page__taxonomy-item" rel="tag">Transformers</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://mohitkpandey.github.io/posts/2020/11/trfm-code/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://mohitkpandey.github.io/posts/2020/11/trfm-code/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://mohitkpandey.github.io/posts/2020/11/trfm-code/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="https://mohitkpandey.github.io/posts/2020/11/TFJS-NER/" class="pagination--pager" title="Named entity recognition with simple Attention
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
      

<div class="page__comments">
  
  
    <h4 class="page__comments-title">Leave a Comment</h4>
    <section id="disqus_thread"></section>
  
</div>
    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
        
        
        
        
        
      
          
          
      
          
          
          
          
        
      
          
          
      
          
          
          
            





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://mohitkpandey.github.io/posts/2020/11/TFJS-NER/" rel="permalink">Named entity recognition with simple Attention
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

    
      <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-11-10T00:00:00-06:00">November 10, 2020</time></p>
    
    
    
    <p class="archive__item-excerpt" itemprop="description"><p>NER implementation hosted within browser using Tensorflow-JS.</p>

<p>Definition from <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Wikipedia</a></p>
<blockquote>
  <p>Named Entity Recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, etc.
See <a href="/posts/2020/11/TFJS-NER/#demo"><b><em>demo</em></b></a> below. Continue reading for model explanation and code.</p>
</blockquote>

</p>
    
    
  </article>
</div>

            
            
          
          
        
      
          
          
      
          
          
          
            





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://mohitkpandey.github.io/posts/2020/11/En-Att-De/" rel="permalink">Encoder - Attention - Decoder
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  10 minute read
	
</p>
    

    
      <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-11-10T00:00:00-06:00">November 10, 2020</time></p>
    
    
    
    <p class="archive__item-excerpt" itemprop="description"><p><strong>Explaining Attention Network in Encoder-Decoder setting using Recurrent Neural Networks</strong></p>

<p>Encoder-Decoder paradigm has become extremely popular in deep learning particularly in the space of natural language processing. Attention modules complement encoder-decoder architecture to make learning more close to humans way. I present a gentle introduction to encode-attend-decode. I  provide motivation for each block and explain the math governing the model. Further, I break down the code into digestible bits for each mathematical equation. While there are good explanations to attention mechanism for machine translation task, I will try to explain the same for a sequence tagging task (Named Entity Recognition).</p>

<div style="width:image width px; font-size:80%; text-align:center;"><img src="../../../../files/images/encode_attend_decode.jpg" width="50%" height="50%" style="padding-bottom:0.5em;" /><b>Encode-Attend-Decode Architecture</b></div>

<p>In the next part of the series, I will use the architecture explained here to solve the problem of <a href="/posts/2020/11/TFJS-NER/">Named Entity Recognition</a></p>

</p>
    
    
  </article>
</div>

            
            
          
          
        
      
          
          
      
          
          
          
          
        
      
          
          
      
          
          
          
          
        
        
        
        
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://mohitkpandey.github.io/posts/2020/11/TFJS-NER/" rel="permalink">Named entity recognition with simple Attention
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

    
      <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-11-10T00:00:00-06:00">November 10, 2020</time></p>
    
    
    
    <p class="archive__item-excerpt" itemprop="description"><p>NER implementation hosted within browser using Tensorflow-JS.</p>

<p>Definition from <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Wikipedia</a></p>
<blockquote>
  <p>Named Entity Recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, etc.
See <a href="/posts/2020/11/TFJS-NER/#demo"><b><em>demo</em></b></a> below. Continue reading for model explanation and code.</p>
</blockquote>

</p>
    
    
  </article>
</div>

        
          
      </div>
    </div>
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/diamondspark"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://mohitkpandey.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Mohit Pandey. </div>

      </footer>
    </div>

    <script src="https://mohitkpandey.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-199829782-1', 'auto');
  ga('send', 'pageview');
</script>






  
  <script type="text/javascript">
  	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  	var disqus_shortname = 'mohitkpandey';

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function() {
  		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  	})();

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function () {
  		var s = document.createElement('script'); s.async = true;
  		s.type = 'text/javascript';
  		s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  		(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  	}());
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






  </body>
</html>

